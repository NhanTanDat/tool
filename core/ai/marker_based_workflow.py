"""
marker_based_workflow.py

Workflow d·ª±a tr√™n Markers:
1. ƒê·ªçc keywords t·ª´ track3_keywords.json
2. G·ªôp keywords tr√πng, t√¨m ki·∫øm & download videos
3. AI ph√¢n t√≠ch videos, l·∫•y danh s√°ch segments "best"
4. V·ªõi m·ªói marker:
   - CH·ªà l·∫•y 2-3 clip xu·∫•t s·∫Øc nh·∫•t (m·∫∑c ƒë·ªãnh 3)
   - keyword tr√πng -> m·ªói marker 1-3 segment KH√ÅC nhau (cursor theo keyword)
   - KH√îNG fill full duration b·∫±ng nhi·ªÅu clip n·ªØa

FIX/IMPROVE:
- Cursor theo keyword (kh√¥ng reset theo marker)
- T√¨m folder keyword robust (slugify + fallback)
- Parse segment time fields an to√†n
- Sort segments theo confidence desc ƒë·ªÉ l·∫•y best tr∆∞·ªõc
"""

from __future__ import annotations

import os
import sys
import json
from pathlib import Path
from typing import Optional, Callable, List, Dict, Any, Tuple
from collections import defaultdict

# Use centralized utilities
from core.utils import setup_paths, load_env, get_gemini_api_key

setup_paths()
load_env()

# =========================
# Helpers
# =========================
def _f(x: Any, default: float = 0.0) -> float:
    try:
        if x is None:
            return default
        return float(x)
    except Exception:
        return default


def _slugify_keyword(s: str) -> str:
    s = (s or "").strip()
    s = s.replace(" ", "_")
    s = "".join(ch if (ch.isalnum() or ch == "_") else "_" for ch in s)
    while "__" in s:
        s = s.replace("__", "_")
    return s.strip("_") or "unknown"


def _keyword_folder_candidates(keyword: str) -> List[str]:
    kw = (keyword or "").strip()
    cands = []
    if kw:
        cands.append(kw)  # ƒë√¥i khi folder gi·ªØ nguy√™n
        cands.append(kw.replace(" ", "_"))
        cands.append(_slugify_keyword(kw))
        cands.append(_slugify_keyword(kw).lower())
    seen = set()
    out = []
    for c in cands:
        if c and c not in seen:
            seen.add(c)
            out.append(c)
    return out


def _find_keyword_folder(resource_folder: Path, keyword: str) -> Path:
    if not resource_folder.exists():
        return resource_folder

    for name in _keyword_folder_candidates(keyword):
        p = resource_folder / name
        if p.exists() and p.is_dir():
            return p

    # Fuzzy: t√¨m folder con ch·ª©a substring ƒë√£ slugify
    key = _slugify_keyword(keyword).lower()
    if key:
        for child in resource_folder.iterdir():
            if child.is_dir() and key in child.name.lower():
                return child

    return resource_folder


def _list_videos(folder: Path) -> List[Path]:
    exts = {".mp4", ".mov", ".avi", ".mkv", ".webm"}
    if not folder.exists():
        return []
    out = []
    for p in folder.rglob("*"):
        if p.is_file() and p.suffix.lower() in exts:
            out.append(p)
    out.sort()
    return out


def _seg_times(seg: Dict[str, Any]) -> Tuple[float, float]:
    """
    Segments c√≥ th·ªÉ d√πng:
    - start_time/end_time
    - start_seconds/end_seconds
    - start/end
    """
    st = seg.get("start_time", seg.get("start_seconds", seg.get("start", 0)))
    en = seg.get("end_time", seg.get("end_seconds", seg.get("end", 0)))
    stf = _f(st, 0.0)
    enf = _f(en, stf)
    if enf < stf:
        enf = stf
    return stf, enf


def _seg_id(seg: Dict[str, Any]) -> str:
    st, en = _seg_times(seg)
    vp = seg.get("video_path", "") or ""
    return f"{vp}|{st:.3f}|{en:.3f}"


def _seg_conf(seg: Dict[str, Any]) -> float:
    return _f(seg.get("confidence", seg.get("score", 0.0)), 0.0)


# =========================
# Workflow Class
# =========================
class MarkerBasedWorkflow:
    """
    Workflow d·ª±a tr√™n markers v·ªõi h·ªó tr·ª£ keywords tr√πng l·∫∑p.
    """

    def __init__(
        self,
        project_path: str,
        data_folder: str,
        resource_folder: str,
        gemini_api_key: Optional[str] = None,
        videos_per_keyword: int = 3,
        clips_per_marker: int = 3,  # <-- NEW: 2-3
        log_callback: Optional[Callable[[str], None]] = None,
    ):
        self.project_path = Path(project_path)
        self.data_folder = Path(data_folder)
        self.resource_folder = Path(resource_folder)
        self.gemini_api_key = gemini_api_key or get_gemini_api_key() or ""
        self.videos_per_keyword = max(1, int(videos_per_keyword))
        self.log_callback = log_callback or print

        # clamp clips_per_marker -> [2..3]
        try:
            cpm = int(clips_per_marker)
        except Exception:
            cpm = 3
        if cpm < 2:
            cpm = 2
        if cpm > 3:
            cpm = 3
        self.clips_per_marker = cpm

        # File paths
        self.keywords_json = self.data_folder / "track3_keywords.json"
        self.dl_links_txt = self.data_folder / "dl_links.txt"
        self.segments_json = self.data_folder / "segments_genmini.json"
        self.cut_list_json = self.data_folder / "cut_list.json"

    def log(self, msg: str):
        self.log_callback(msg)

    def load_keywords(self) -> List[Dict[str, Any]]:
        if not self.keywords_json.exists():
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y: {self.keywords_json}")

        with open(self.keywords_json, "r", encoding="utf-8-sig") as f:
            data = json.load(f)

        return data.get("keywords", [])

    def group_keywords(self, keywords: List[Dict]) -> Dict[str, List[Dict]]:
        groups = defaultdict(list)
        for kw in keywords:
            text = (kw.get("keyword", "") or "").strip()
            if text:
                groups[text].append(kw)
        return dict(groups)

    # =========================
    # STEP 1
    # =========================
    def step1_analyze_keywords(self) -> Dict[str, List[Dict]]:
        self.log("\n" + "=" * 50)
        self.log("  B∆Ø·ªöC 1: PH√ÇN T√çCH KEYWORDS")
        self.log("=" * 50)

        if not self.keywords_json.exists():
            self.log(f"‚ùå Kh√¥ng t√¨m th·∫•y: {self.keywords_json}")
            self.log("   H√£y ch·∫°y 'ƒê·ªçc Markers' tr∆∞·ªõc!")
            return {}

        try:
            keywords = self.load_keywords()
            self.log(f"\nüìã T·ªïng s·ªë markers: {len(keywords)}")

            groups = self.group_keywords(keywords)
            self.log(f"üìã Keywords unique: {len(groups)}\n")

            for kw_text, markers in groups.items():
                self.log(f"   [{len(markers)}x] {kw_text}")
                for m in markers:
                    self.log(f"        ‚îî‚îÄ {m.get('start_timecode', '')} ({_f(m.get('duration_seconds', 0)):.1f}s)")

            self.log(f"\nüéØ C·∫•u h√¨nh: clips_per_marker={self.clips_per_marker} (m·ªói marker l·∫•y 2-3 clip best)")
            return groups
        except Exception as e:
            self.log(f"‚ùå L·ªói: {e}")
            return {}

    # =========================
    # STEP 2
    # =========================
    def step2_download_videos(self, keyword_groups: Dict[str, List[Dict]]) -> bool:
        self.log("\n" + "=" * 50)
        self.log("  B∆Ø·ªöC 2: DOWNLOAD VIDEOS")
        self.log("=" * 50)

        if not keyword_groups:
            self.log("‚ùå Kh√¥ng c√≥ keywords")
            return False

        unique_keywords = list(keyword_groups.keys())
        self.log(f"\nüîç S·∫Ω t√¨m videos cho {len(unique_keywords)} keywords unique")

        try:
            from core.downloadTool.get_link import _search_youtube_for_keyword
        except ImportError as e:
            self.log(f"‚ùå Kh√¥ng import ƒë∆∞·ª£c get_link: {e}")
            return False

        lines = []
        total_links = 0
        global_seen = set()

        for i, kw in enumerate(unique_keywords):
            count_needed_markers = len(keyword_groups[kw])

            # V·ªõi y√™u c·∫ßu ch·ªâ 2-3 clips/marker:
            # N√™n t·∫£i d∆∞ 1 ch√∫t ƒë·ªÉ AI c√≥ ngu·ªìn ch·ªçn
            videos_to_get = max(self.videos_per_keyword, min(10, count_needed_markers + 2))

            self.log(f"\n[{i + 1}/{len(unique_keywords)}] \"{kw}\"")
            self.log(f"   Markers: {count_needed_markers}, download: {videos_to_get} videos")

            try:
                search_n = videos_to_get * 6
                candidates = _search_youtube_for_keyword(kw, max_results=search_n)

                urls_ok = []
                for c in candidates:
                    url = (c.get("url", "") or "").strip()
                    if not url or url in global_seen:
                        continue
                    global_seen.add(url)
                    urls_ok.append(url)
                    if len(urls_ok) >= videos_to_get:
                        break

                lines.append(kw)
                for url in urls_ok:
                    lines.append(url)
                    total_links += 1
                lines.append("")

                self.log(f"   ‚úì T√¨m ƒë∆∞·ª£c {len(urls_ok)} videos")
            except Exception as e:
                self.log(f"   ‚ùå L·ªói: {e}")
                lines.append(kw)
                lines.append("")

        self.dl_links_txt.write_text("\n".join(lines), encoding="utf-8")
        self.log(f"\n‚úì ƒê√£ l∆∞u {total_links} links ‚Üí {self.dl_links_txt.name}")

        self.log("\nüì• ƒêang download videos...")
        self.resource_folder.mkdir(parents=True, exist_ok=True)

        try:
            from core.downloadTool.down_by_yt import download_main

            download_main(
                parent_folder=str(self.resource_folder),
                txt_name=str(self.dl_links_txt),
                _type="mp4",
            )
            self.log(f"‚úì ƒê√£ download videos v√†o {self.resource_folder}")
            return True
        except ImportError as e:
            self.log(f"‚ö† Kh√¥ng import ƒë∆∞·ª£c download function: {e}")
            self.log(f"   Ch·∫°y th·ªß c√¥ng v·ªõi: {self.dl_links_txt}")
            return True
        except Exception as e:
            self.log(f"‚ö† L·ªói download: {e}")
            return True

    # =========================
    # STEP 3
    # =========================
    def step3_ai_analyze(self, keyword_groups: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        B∆∞·ªõc 3: AI ph√¢n t√≠ch videos
        Tr·∫£ v·ªÅ dict: keyword -> list of segments (ƒë√£ sort best tr∆∞·ªõc)
        """
        self.log("\n" + "=" * 50)
        self.log("  B∆Ø·ªöC 3: AI PH√ÇN T√çCH VIDEOS")
        self.log("=" * 50)

        if not self.gemini_api_key:
            self.log("‚ö† Kh√¥ng c√≥ GEMINI_API_KEY, b·ªè qua AI analyze")
            return {}

        key_preview = self.gemini_api_key[:8] + "..." + self.gemini_api_key[-4:]
        self.log(f"\n‚úì API Key: {key_preview}")

        keyword_segments: Dict[str, List[Dict]] = {}

        try:
            from core.ai.genmini_analyze import analyze_video_for_keyword
        except ImportError:
            self.log("‚ö† Kh√¥ng import ƒë∆∞·ª£c core.ai.genmini_analyze")
            return {}

        for kw_text, markers in keyword_groups.items():
            marker_count = len(markers)

            # M·ªói marker c·∫ßn 2-3 clip best => t·ªïng c·∫ßn ~ marker_count * clips_per_marker (+ buffer)
            total_needed = marker_count * self.clips_per_marker
            buffer = 3
            target_total = max(6, total_needed + buffer)

            self.log(f"\nüîç \"{kw_text}\"")
            self.log(f"   Markers: {marker_count}")
            self.log(f"   Target segments t·ªïng: {target_total} (ƒë·ªÉ ƒë·ªß c·∫•p cho marker tr√πng)")

            kw_folder = _find_keyword_folder(self.resource_folder, kw_text)
            videos = _list_videos(kw_folder)

            if not videos and kw_folder != self.resource_folder:
                videos = _list_videos(self.resource_folder)

            if not videos:
                self.log("   ‚ö† Kh√¥ng t√¨m th·∫•y video cho keyword n√†y")
                continue

            use_videos = videos[: self.videos_per_keyword]
            self.log(f"   Folder d√πng: {kw_folder.name}")
            self.log(f"   Videos d√πng: {len(use_videos)}/{len(videos)}")

            # Chia quota segments cho t·ª´ng video ƒë·ªÉ t·ªïng ƒë·∫°t target_total
            per_video = max(3, min(8, (target_total // max(1, len(use_videos))) + 1))

            all_segments: List[Dict[str, Any]] = []

            for video in use_videos:
                try:
                    self.log(f"   Analyzing: {video.name} (max {per_video} segments)...")
                    segments = analyze_video_for_keyword(
                        video_path=str(video),
                        keyword=kw_text,
                        max_segments=per_video,
                        api_key=self.gemini_api_key,
                    )

                    fixed = []
                    for seg in segments or []:
                        st, en = _seg_times(seg)
                        if en <= st:
                            continue
                        seg["video_path"] = str(video)
                        seg["video_name"] = video.name
                        seg["start_time"] = st
                        seg["end_time"] = en
                        fixed.append(seg)

                    all_segments.extend(fixed)
                    self.log(f"      ‚Üí {len(fixed)} segments")
                except Exception as e:
                    self.log(f"      ‚ùå L·ªói: {e}")

            # De-dup
            uniq = []
            seen_ids = set()
            for s in all_segments:
                sid = _seg_id(s)
                if sid in seen_ids:
                    continue
                seen_ids.add(sid)
                uniq.append(s)

            # Sort BEST first (confidence desc, r·ªìi duration desc)
            def _sort_key(s: Dict[str, Any]):
                st, en = _seg_times(s)
                dur = max(0.0, en - st)
                return (_seg_conf(s), dur)

            uniq.sort(key=_sort_key, reverse=True)

            # Gi·ªØ top ƒë·ªÉ kh·ªèi qu√° nhi·ªÅu (ƒë·ªß c·∫•p marker)
            uniq = uniq[: max(target_total, 10)]

            keyword_segments[kw_text] = uniq
            self.log(f"   ‚úì T·ªïng unique (sorted best): {len(uniq)} segments")

        # Save for debug
        try:
            with open(self.segments_json, "w", encoding="utf-8") as f:
                json.dump(keyword_segments, f, ensure_ascii=False, indent=2)
            self.log(f"\n‚úì Saved segments ‚Üí {self.segments_json.name}")
        except Exception as e:
            self.log(f"‚ö† Kh√¥ng l∆∞u ƒë∆∞·ª£c segments_genmini.json: {e}")

        return keyword_segments

    # =========================
    # STEP 4
    # =========================
    def step4_generate_cut_list(
        self,
        keyword_groups: Dict[str, List[Dict]],
        keyword_segments: Dict[str, List[Dict]],
    ) -> bool:
        """
        B∆∞·ªõc 4: Sinh cut_list.json
        Y√äU C·∫¶U:
        - M·ªói marker ch·ªâ l·∫•y 2-3 clip best
        - keyword tr√πng -> m·ªói marker l·∫•y segment kh√°c nhau (cursor theo keyword)
        - KH√îNG fill full duration n·ªØa
        """
        self.log("\n" + "=" * 50)
        self.log("  B∆Ø·ªöC 4: SINH CUT LIST (2-3 CLIPS / MARKER)")
        self.log("=" * 50)

        all_videos = _list_videos(self.resource_folder)
        self.log(f"\nVideos trong resource: {len(all_videos)}")

        keywords = self.load_keywords()

        # Cursor theo keyword
        seg_cursor: Dict[str, int] = defaultdict(int)

        cuts: List[Dict[str, Any]] = []

        for kw in keywords:
            idx = int(kw.get("index", 0))
            kw_text = (kw.get("keyword", "") or "").strip()
            if not kw_text:
                continue

            timeline_start = _f(kw.get("start_seconds", 0), 0.0)
            timeline_end = _f(kw.get("end_seconds", timeline_start), timeline_start)
            marker_duration = _f(
                kw.get("duration_seconds", max(0.0, timeline_end - timeline_start)),
                max(0.0, timeline_end - timeline_start),
            )
            marker_duration = max(0.0, marker_duration)

            # N·∫øu end_seconds kh√¥ng c√≥ / l·ªói, t·ª± suy ra end theo duration
            if timeline_end <= timeline_start and marker_duration > 0:
                timeline_end = timeline_start + marker_duration

            self.log(f"\n[{idx}] \"{kw_text}\" @ {kw.get('start_timecode', '')} ({marker_duration:.1f}s)")

            segments = keyword_segments.get(kw_text, []) or []

            marker_clips: List[Dict[str, Any]] = []
            current_pos = timeline_start

            # M·ª•c ti√™u: 2-3 clips/marker (m·∫∑c ƒë·ªãnh 3)
            target = self.clips_per_marker

            used_segment_ids = set()
            picked = 0
            safety = 0

            while picked < target and safety < 500:
                safety += 1

                if not segments:
                    break

                cur = seg_cursor[kw_text]
                if cur >= len(segments):
                    break

                seg = segments[cur]
                seg_cursor[kw_text] += 1

                sid = _seg_id(seg)
                if sid in used_segment_ids:
                    continue
                used_segment_ids.add(sid)

                st, en = _seg_times(seg)
                seg_dur = max(0.0, en - st)
                if seg_dur <= 0.6:
                    continue

                # KH√îNG v∆∞·ª£t qu√° marker end (ƒë·ªÉ tr√°nh ƒë√® marker sau)
                remaining = max(0.0, timeline_end - current_pos) if timeline_end > timeline_start else seg_dur
                if remaining < 0.8:
                    break

                clip_dur = min(seg_dur, remaining)

                # N·∫øu clip qu√° ng·∫Øn do remaining, b·ªè qua ƒë·ªÉ kh√¥ng ra clip ‚Äúl·ª•i‚Äù
                # (v√¨ b·∫°n mu·ªën clip xu·∫•t s·∫Øc 2-4s)
                if clip_dur < 1.8:
                    # n·∫øu ch∆∞a pick ƒë∆∞·ª£c g√¨, v·∫´n c·ªë pick 1 clip ƒë·ªÉ kh·ªèi r·ªóng
                    if picked == 0:
                        clip_dur = seg_dur
                    else:
                        continue

                marker_clips.append(
                    {
                        "video_path": seg.get("video_path", ""),
                        "video_name": seg.get("video_name", ""),
                        "clip_start": float(st),
                        "clip_end": float(st + clip_dur),
                        "timeline_pos": float(current_pos),
                        "duration": float(clip_dur),
                        "source": "ai_best",
                        "confidence": float(_seg_conf(seg)),
                        "description": seg.get("description", seg.get("script_notes", "")),
                    }
                )

                current_pos += clip_dur
                picked += 1

            # N·∫øu kh√¥ng c√≥ segment AI -> fallback 1 clip ng·∫Øn (v·∫´n gi·ªØ logic t·ªëi thi·ªÉu)
            if not marker_clips and all_videos:
                fallback_video = all_videos[abs(hash(kw_text)) % len(all_videos)]
                # ch·ªçn 1 ƒëo·∫°n 3s random-ish theo hash
                base = (abs(hash(kw_text + str(idx))) % 50)  # 0..49s
                clip_dur = 3.0
                marker_clips.append(
                    {
                        "video_path": str(fallback_video),
                        "video_name": fallback_video.name,
                        "clip_start": float(base),
                        "clip_end": float(base + clip_dur),
                        "timeline_pos": float(timeline_start),
                        "duration": float(clip_dur),
                        "source": "fallback_one",
                        "confidence": 0.1,
                        "description": "fallback",
                    }
                )

            if marker_clips:
                self.log(f"   ‚Üí Picked {len(marker_clips)} clips (target {target})")
            else:
                self.log("   ‚ö† Kh√¥ng c√≥ clip n√†o cho marker n√†y")

            cuts.append(
                {
                    "index": idx,
                    "keyword": kw_text,
                    "timeline_start": timeline_start,
                    "timeline_end": timeline_end,
                    "timeline_duration": marker_duration,
                    "clips": marker_clips,
                    "clip_count": len(marker_clips),
                }
            )

        total_clips = sum(c.get("clip_count", 0) for c in cuts)
        markers_with_clips = sum(1 for c in cuts if c.get("clip_count", 0) > 0)
        ai_clips = sum(sum(1 for clip in c.get("clips", []) if clip.get("source") == "ai_best") for c in cuts)

        cut_data = {
            "count": len(cuts),
            "total_clips": total_clips,
            "markers_with_clips": markers_with_clips,
            "ai_clips": ai_clips,
            "clips_per_marker": self.clips_per_marker,
            "cuts": cuts,
        }

        with open(self.cut_list_json, "w", encoding="utf-8") as f:
            json.dump(cut_data, f, ensure_ascii=False, indent=2)

        self.log("\n" + "=" * 50)
        self.log("  ‚úì CUT LIST HO√ÄN TH√ÄNH")
        self.log("=" * 50)
        self.log(f"   T·ªïng markers:      {len(cuts)}")
        self.log(f"   Markers c√≥ clips:  {markers_with_clips}")
        self.log(f"   T·ªïng clips:        {total_clips}")
        self.log(f"   AI clips:          {ai_clips}")
        self.log(f"   File:              {self.cut_list_json.name}")

        return True

    # =========================
    # RUN
    # =========================
    def run_full_workflow(self, skip_download: bool = False) -> bool:
        self.log("\n" + "=" * 60)
        self.log("  üöÄ MARKER-BASED WORKFLOW")
        self.log("=" * 60)

        self.data_folder.mkdir(parents=True, exist_ok=True)

        keyword_groups = self.step1_analyze_keywords()
        if not keyword_groups:
            return False

        if not skip_download:
            self.step2_download_videos(keyword_groups)

        keyword_segments = self.step3_ai_analyze(keyword_groups)

        if not self.step4_generate_cut_list(keyword_groups, keyword_segments):
            return False

        self.log("\n" + "=" * 60)
        self.log("  ‚úì‚úì‚úì WORKFLOW HO√ÄN TH√ÄNH ‚úì‚úì‚úì")
        self.log("=" * 60)
        self.log("\nüìã B∆∞·ªõc ti·∫øp theo:")
        self.log("   Ch·∫°y executeCuts.jsx trong Premiere ƒë·ªÉ ƒë·ªï clips v√†o V4\n")

        return True


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Marker-based Workflow")
    parser.add_argument("--project", required=True)
    parser.add_argument("--data-folder", required=True)
    parser.add_argument("--resource-folder", required=True)
    parser.add_argument("--videos-per-keyword", type=int, default=3)
    parser.add_argument("--clips-per-marker", type=int, default=3)  # 2..3
    parser.add_argument("--skip-download", action="store_true")
    parser.add_argument("--gemini-key")

    args = parser.parse_args()

    workflow = MarkerBasedWorkflow(
        project_path=args.project,
        data_folder=args.data_folder,
        resource_folder=args.resource_folder,
        gemini_api_key=args.gemini_key,
        videos_per_keyword=args.videos_per_keyword,
        clips_per_marker=args.clips_per_marker,
    )

    ok = workflow.run_full_workflow(skip_download=args.skip_download)
    sys.exit(0 if ok else 1)


if __name__ == "__main__":
    main()
